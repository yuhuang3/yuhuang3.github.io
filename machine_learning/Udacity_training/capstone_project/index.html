<h1> Predicting Hospitalized Time of Covid-19 Patients</h1>
<h2> Supervised Machine Learning in Healthcare </h2>
<div>
    <img src="./images/scene.jpg" width="50%" alt="Yu"> 
</div>
<h3> Yu Huang </h3>

<h2> 1. Intruduction </h2>
<p>
The outbreak of Covid-19 pandemic in 2020 caused tremendous equipment, materials deficiency and beds shortage problems in healthcare system across the U.S. [1]. The hospitals not only need to take care of the routine patients, but also need to take care of the sudden increased Covid-19 patients. A good planning and management of a hospital system becomes very important.
This article presents how new methods to use machine-learning to predict how long (in days) a Covid-19 patient needs to stay in a hospital at the time of admission. This can help hospital professionals to make an optimized planning for patient treatment and resources (e.g., room, bed, etc.) allocation. This also could reduce the amount of hospital visitors and so decrease the chance of staff and visitor infection.
The rest of this article is arranged as follows:
</p>
<ul>
    <li>Data understanding</li>
    <li>Data preparation</li>
    <li>Modeling</li>
    <li>Model evaluation</li>
    <li>Deployment </li>
</ul>


<h2> 1. Data Understanding </h2>

The dataset is obtained from Kaggle's website [1]. There are three files used in this article:
train_data.csv: containing features related to patient, hospital and Length of stay (label)
test_data.csv: containing features related to patient, hospital. Need to predict the "length" of stay for each case id
train_data_dictionary.csv: containing the information of the features in train and test files.

As shown in Figure 1, there are total 17 features and 1 label column (Stay) in this dataset.

<br>
<img src="./images/covid19_data_transposed_view.png" width="70%" alt="top 10 languages in 2017">
<br>
<center> Figure 1: Transposed view of the train dataset. </center>
<br>
<p>
    Figure 2 shows the distribution of the following numeric features:
<ul>
    <li>case_id</li>
    <li>Hospital_code</li>
    <li>City_Code_Hospital</li>
    <li>Available Extra Rooms in Hospital</li>
    <li>Bed Grade</li>
    <li>patientid</li>
    <li>City_Code_Patient</li>
    <li>Visitors with Patient</li>
    <li>Admission_Deposit</li>
 </ul>   
    It can be seen that the case_id feature values are uniformly distributed 
    over bins because they are unique sequential numbers. 
    This feature can be dropped due to lack of prediction power.    
</p>

<br>
<img src="./images/numeric_features_distribution.png" width="50%" alt="top 10 languages in 2017">
<br>
<center> Figure 2: Distribution of numeric features. </center>
<br>

<p>
    Figure 3 shows the patient visit distribution. 
    We can see that many patients revisited the hospital many times 
    (from 10 up to 50). So patient IDs are important in prediction.
</p>

<br>
<img src="./images/patient_visits_distribution.png" width="40%" alt="top 10 languages in 2017">
<br>
<center> Figure 3: Patient visit distribution. </center>
<br>

<p>
    Figure 4 shows the distribution of the categorical features and label column:
<ul>
    <li>Hospital type code</li>
    <li>Hospital region code</li>
    <li>Department</li>
    <li>Ward Type</li>
    <li>Ward Facility Code</li>
    <li>Type of Admission</li>
    <li>Severity of Illness</li>
    <li>Age</li>
    <li>Stay (Label)</li>
</ul>

We can see that the distribution of the labels is significantly skewed to 
the right. In other words, the data is not balanced. 
There are very few data samples in the categories from '41–50' to '61–70'. 
This will have significant negative impact on prediction power.
</p>

<br>
<img src="./images/categorical_features_distribution.png" width="50%" alt="top 10 languages in 2017">
<br>
<center> Figure 4: Distribution of categorical features. </center>
<br>

<h2> 2. Data preparation </h2>

With data understanding, The next step is to explore, 
clean and transform the collected raw dataset into appropriate format 
so that the transformed data can be effectively consumed by a target machine learning model.

<h3>2.1. Handling missing data</h3>

As shown in Figure 5, there are 113 missing data in the Bed Grade feature column 
and 4,532 missing data in the City_Code_Patient feature column. 
The total number of missing data is relatively small (total of 4,645) 
compared with the dataset size of 318,438 rows. 
In this case we can either drop the rows with missing data or replace missing data with 0. 
I choose to replace the missing data with 0 in order to be able to predict results 
for the test_data dataset with missing feature values in deployment. 
Refer to the DataCleaning class in data_preprocessing.py for details [6].

<p>
train_data.isnull().sum()
</p>

<br>
<img src="./images/missing_data_count.png" width="50%" alt="top 10 languages in 2017">
<br>
<center> Figure 5: Count of missing data. </center>
<br>

<h3>2.2 Dropping features (columns) without prediction power</h3>
As described before, the case_id feature doesn't have prediction power, 
so it is dropped in this project (see the DataCleaning class in data_preprocessing.py[6]).

<h3>2.3 Categorical Encoding</h3>
<h4>2.3.1. Categorical label encoding</h4>
The label column Stay in this dataset is categorical. It must be transformed into numbers for categorical feature target encoding [7] and deep learning model. The LabelEncoder algorithm [8] is used for the transformation (see TargetEncoding and OneHotEncoding classes in data_preprocessing.py[6]).
<h4>2.3.2 Categorical feature target encoding</h4>
The advantage of target encoding [7] is that it does not increase the dimensionality of the dataset. It has been used in this project for transforming categorical features into numbers for ensemble machine learning models XGBoost [2] and Random Forest [3] because these models don't work well with one-hot encoding due to high dimensionality. See the TargetEncoding class in data_preprocessing.py[6].
<h4>2.3.3. Categorical feature one-hot encoding</h4>
For comparison, the popular one-hot encoding method is used for transforming categorical features as well for deep learning model (see the OneHotEncoding class in data_preprocessing.py[6]).
<h4>2.3.4. Other categorical feature transformation</h4>
I noticed that the categorical Age feature (e.g. "21–30") has more prediction power once it is transformed into numbers because the order of ages makes difference. In this project each range of age (e.g., "21–30") has been transformed into an average number like (21+30)/2 = 25.5 (see the OneHotEncoding class in data_preprocessing.py[6]).
<h3>2.4. Feature Normalization</h3>
The numeric features are normalized into the range of [-1, 1] for deep learning (see the FeatureNormorlization class in data_preprocessing.py[6]).
<h3>2.5. Data Splitting</h3>
Finally, the preprocessed dataset is divided into two subsets: one for model training and the other for model evaluation.
<h3>2.6. Data Preprocessing Pipelines</h3>
The data preparation steps 2.1–2.5 have been combined into data preprocessing pipelines for convenience:
<ul>
    <li>target_encoding_preprocessing</li>
    <li>target_encoding_preprocessing_for_prediction</li>
    <li>onehot_encoding_preprocessing</li>
    <li>onehot_encoding_preprocessing_for_prediction</li>
</ul>
<br>

See data_preprocessing.py for details [6].




<h2> 3. Modeling </h2>

<h2> 4. Model evaluation </h2>

<h2> 5. Deployment </h2>

<h2> Summary </h2>

<h2> References </h2>



